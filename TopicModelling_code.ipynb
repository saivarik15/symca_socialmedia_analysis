{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import contractions\n",
    "import string\n",
    "import re \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path_SYMCA ='./Dataset/SYMCA2.parquet'\n",
    "\n",
    "tweets_df = pd.read_parquet(file_path_SYMCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[tweets_df['Handle'] == '@SouthYorksMCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataCleaning and Preprocessing\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "#converting every row of the column into lower case \n",
    "tweets_df.Tweet=tweets_df.Tweet.apply(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing Accent Characters\n",
    "def standardize_accented_chars(text):\n",
    " return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#testing the function on a single sample for explaination\n",
    "print(standardize_accented_chars('Sómě words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.'))\n",
    "#standardizing accented characters for every row\n",
    "tweets_df.Tweet=tweets_df.Tweet.apply(standardize_accented_chars)\n",
    "\n",
    "print(tweets_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove URL from the tweets\n",
    "def get_number_of_urls(documents):\n",
    "    print(\"{:.2f}% of documents contain urls\".format(sum\n",
    "(documents.apply(lambda x:x.find('http'))>0)/len\n",
    "(documents)*100))\n",
    "# Passing the 'Tweets' column of the dataframe as the argument\n",
    "print(get_number_of_urls(tweets_df.Tweet)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the urls from text using the below-mentioned function\n",
    "def remove_url(text):\n",
    " return re.sub(r'https?:\\S*', '', text)\n",
    "#testing the function on a single sample for explaination\n",
    "print(remove_url('using https://www.google.com/ as an example'))\n",
    "#removing urls from every row\n",
    "tweets_df.Tweet=tweets_df.Tweet.apply(remove_url)\n",
    "print(tweets_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding Contractions : convert each contraction into its expanded form \n",
    "def expand_contractions(text):\n",
    "    expanded_words = [] \n",
    "    for word in text.split():\n",
    "       expanded_words.append(contractions.fix(word)) \n",
    "    return ' '.join(expanded_words)\n",
    "#testing the function on a single sample for explaination\n",
    "print(expand_contractions(\"Don't is same as do not\"))\n",
    "#expanding contractions for every row\n",
    "tweets_df.Tweet=tweets_df.Tweet.apply(expand_contractions)\n",
    "print(tweets_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the function to remove mentions and hashtags\n",
    "def remove_mentions_and_tags(text):\n",
    "    text = re.sub(r'@\\S*', '', text)\n",
    "    return re.sub(r'#\\S*', '', text)\n",
    "\n",
    "# Testing the function on a single sample\n",
    "print(remove_mentions_and_tags('Some random @abc and #def'))\n",
    "\n",
    "# Removing mentions and tags from every row in the DataFrame\n",
    "tweets_df['Tweet'] = tweets_df['Tweet'].apply(remove_mentions_and_tags)\n",
    "print(tweets_df['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def keep_only_alphabet(text):\n",
    "    return re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "\n",
    "# Testing the function on a single sample\n",
    "print(keep_only_alphabet(' Just a bit more $$processing required.Just a bit!!!'))\n",
    "\n",
    "# For all the rows in the DataFrame\n",
    "tweets_df['Tweet'] = tweets_df['Tweet'].apply(keep_only_alphabet)\n",
    "print(tweets_df['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stopwords(Default+Custom) and Removing Short Words\n",
    "def remove_stopwords(text,nlp,custom_stop_words=None,\n",
    "remove_small_tokens=True,min_len=2):\n",
    "    # if custom stop words are provided, then add them to default stop words list\n",
    "    if custom_stop_words:\n",
    "        nlp.Defaults.stop_words |= custom_stop_words\n",
    "    \n",
    "    filtered_sentence =[] \n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.is_stop == False: \n",
    "            \n",
    "            # if small tokens have to be removed, then select only those which are longer than the min_len \n",
    "            if remove_small_tokens:\n",
    "                if len(token.text)>min_len:\n",
    "                    filtered_sentence.append(token.text)\n",
    "            else:\n",
    "                filtered_sentence.append(token.text)\n",
    "    # if after the stop word removal, words are still left in the sentence, then return the sentence as a string else return null \n",
    "    return \" \".join(filtered_sentence) if len(filtered_sentence)>0 else None\n",
    "\n",
    "#creating a spaCy object. \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "#removing stop-words and short words from every row\n",
    "tweets_df.Tweet=tweets_df.Tweet.apply(lambda x:remove_stopwords(x,nlp,{\"SYMCA\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Tweet'].fillna(\"\", inplace=True)  # Fill missing values with empty strings\n",
    "\n",
    "#LEMETIZATION\n",
    "\n",
    "def lemmatize(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = []\n",
    "    for token in doc:\n",
    "        lemmatized_text.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "# Testing the function on a single sample\n",
    "print(lemmatize('Reading NLP blog is fun.', nlp))\n",
    "\n",
    "# Performing lemmatization on every row\n",
    "tweets_df['Tweet'] = tweets_df['Tweet'].apply(lambda x: lemmatize(x, nlp))\n",
    "print(tweets_df['Tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Document Matrix and Dictionary:\n",
    "def generate_tokens(tweet):\n",
    "    words=[]\n",
    "    for word in tweet.split(' '):\n",
    "    # using the if condition because we introduced extra spaces during text cleaning\n",
    "        if word!='':\n",
    "            words.append(word)\n",
    "        return words\n",
    "#storing the generated tokens in a new column named 'words'\n",
    "tweets_df['tokens']=tweets_df.Tweet.apply(generate_tokens)\n",
    "print(tweets_df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionary(words):\n",
    "    return corpora.Dictionary(words)\n",
    "#passing the dataframe column having tokens as the argument\n",
    "id2word=create_dictionary(tweets_df.tokens)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_matrix(tokens,id2word):\n",
    "    corpus = []\n",
    "    for text in tokens:\n",
    "            corpus.append(id2word.doc2bow(text))\n",
    "    return corpus\n",
    "\n",
    "#print(tweets_df.tokens[1])\n",
    "print(id2word)\n",
    "corpus=create_document_matrix(tweets_df.tokens,id2word)\n",
    "print(tweets_df.tokens)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    " id2word=id2word,\n",
    " num_topics=10,\n",
    " random_state=100,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genrating LDA topics\n",
    "def get_lda_topics(model, num_topics, top_n_words):\n",
    "     word_dict = {}\n",
    "     for i in range(num_topics):\n",
    "         word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in model.show_topic(i, topn = top_n_words)];\n",
    " \n",
    "     return pd.DataFrame(word_dict)\n",
    "get_lda_topics(lda_model,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_lda_topics(model, num_topics, top_n_words):\n",
    "    topic_data = []\n",
    "    for i in range(num_topics):\n",
    "        topic_words = [(word, weight) for word, weight in model.show_topic(i, topn=top_n_words)]\n",
    "        topic_data.append(topic_words)\n",
    " \n",
    "    return pd.DataFrame(topic_data, columns=[f\"Keyword #{i+1}\" for i in range(top_n_words)])\n",
    "\n",
    "# Assuming you have already trained your LDA model and named it lda_model\n",
    "# Adjust the number of topics and top n words accordingly\n",
    "num_topics = 10\n",
    "top_n_words = 10\n",
    "\n",
    "# Get the LDA topics with keywords and weights using your modified function\n",
    "lda_topics_df = get_lda_topics(lda_model, num_topics, top_n_words)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(lda_topics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Assuming you have already defined lda_model, corpus, and id2word\n",
    "\n",
    "# Create the LDA visualization\n",
    "lda_vis_data = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(lda_vis_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
